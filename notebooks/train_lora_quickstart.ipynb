{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperGen LoRA Training Quickstart\n",
    "\n",
    "This notebook demonstrates how to train a LoRA on a diffusion model using HyperGen.\n",
    "\n",
    "We'll:\n",
    "1. Install dependencies\n",
    "2. Download a small dataset from HuggingFace\n",
    "3. Train a LoRA in just 5 lines of code\n",
    "4. Generate images with the trained LoRA\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU with at least 12GB VRAM (e.g., T4, RTX 3060, or better)\n",
    "- Or use Google Colab with GPU runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HyperGen and dependencies\n",
    "!pip install -q torch torchvision diffusers transformers accelerate peft pillow datasets tqdm\n",
    "\n",
    "# If running locally, install hypergen from source\n",
    "# !pip install -e ..\n",
    "\n",
    "# For Colab/remote environments\n",
    "!pip install -q git+https://github.com/ntegrals/hypergen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from HuggingFace\n",
    "\n",
    "We'll use a small Pokemon dataset for this example. It's perfect for quick experimentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load a small Pokemon dataset from HuggingFace\n",
    "print(\"Downloading dataset...\")\n",
    "hf_dataset = load_dataset(\"lambdalabs/naruto-blip-captions\", split=\"train\")\n",
    "\n",
    "# Take only first 50 images for quick training\n",
    "hf_dataset = hf_dataset.select(range(min(50, len(hf_dataset))))\n",
    "\n",
    "print(f\"Loaded {len(hf_dataset)} images\")\n",
    "print(f\"Sample: {hf_dataset[0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to local folder in HyperGen format\n",
    "dataset_dir = Path(\"./training_data\")\n",
    "dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "for idx, item in enumerate(hf_dataset):\n",
    "    # Save image\n",
    "    image_path = dataset_dir / f\"image_{idx:04d}.png\"\n",
    "    item[\"image\"].save(image_path)\n",
    "    \n",
    "    # Save caption\n",
    "    caption_path = dataset_dir / f\"image_{idx:04d}.txt\"\n",
    "    with open(caption_path, \"w\") as f:\n",
    "        f.write(item[\"text\"])\n",
    "\n",
    "print(f\"✓ Dataset saved to {dataset_dir}\")\n",
    "print(f\"✓ Total images: {len(list(dataset_dir.glob('*.png')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some samples from the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(8):\n",
    "    if idx < len(hf_dataset):\n",
    "        axes[idx].imshow(hf_dataset[idx][\"image\"])\n",
    "        axes[idx].set_title(hf_dataset[idx][\"text\"][:50] + \"...\", fontsize=8)\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train LoRA - Just 5 Lines!\n",
    "\n",
    "This is where the magic happens. HyperGen makes it incredibly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypergen import model, dataset\n",
    "\n",
    "# 1. Load model (using SDXL Turbo for speed)\n",
    "print(\"Loading model...\")\n",
    "m = model.load(\"stabilityai/sdxl-turbo\")\n",
    "m.to(\"cuda\")\n",
    "\n",
    "# 2. Load dataset\n",
    "print(\"\\nLoading dataset...\")\n",
    "ds = dataset.load(\"./training_data\")\n",
    "print(f\"Dataset size: {len(ds)} images\")\n",
    "\n",
    "# 3. Train LoRA - that's it!\n",
    "print(\"\\nStarting training...\\n\")\n",
    "lora = m.train_lora(\n",
    "    ds,\n",
    "    steps=100,              # Quick training for demo\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=1,\n",
    "    rank=16,                # LoRA rank\n",
    "    alpha=32,               # LoRA alpha\n",
    "    resolution=512,         # Training resolution\n",
    "    output_dir=\"./lora_checkpoints\",\n",
    "    save_steps=50,          # Save every 50 steps\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Images with Trained LoRA\n",
    "\n",
    "Now let's test our trained LoRA by generating some images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained LoRA weights\n",
    "from peft import PeftModel\n",
    "\n",
    "# The LoRA is already in the model from training, but if you want to load from checkpoint:\n",
    "# checkpoint_path = \"./lora_checkpoints/checkpoint-100\"\n",
    "# m.pipeline.unet = PeftModel.from_pretrained(m.pipeline.unet, checkpoint_path)\n",
    "\n",
    "print(\"LoRA loaded and ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images with the trained LoRA\n",
    "prompts = [\n",
    "    \"naruto uzumaki standing in a forest\",\n",
    "    \"sasuke uchiha with lightning\",\n",
    "    \"kakashi hatake reading a book\",\n",
    "    \"sakura haruno in action pose\",\n",
    "]\n",
    "\n",
    "print(\"Generating images...\")\n",
    "images = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    image = m.generate(\n",
    "        prompt,\n",
    "        num_inference_steps=4,  # SDXL Turbo uses few steps\n",
    "        guidance_scale=0.0,     # Turbo doesn't use guidance\n",
    "    )\n",
    "    images.append(image[0] if isinstance(image, list) else image)\n",
    "\n",
    "print(\"✓ Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (image, prompt) in enumerate(zip(images, prompts)):\n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(prompt, fontsize=10, wrap=True)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generated_images.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Images saved to generated_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare: Before vs After LoRA\n",
    "\n",
    "Let's compare generations from the base model vs our LoRA-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate without LoRA (base model)\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "print(\"Loading base model for comparison...\")\n",
    "base_model = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "test_prompt = \"naruto uzumaki in ninja outfit\"\n",
    "\n",
    "print(f\"\\nGenerating with base model: '{test_prompt}'\")\n",
    "base_image = base_model(\n",
    "    test_prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0.0,\n",
    ").images[0]\n",
    "\n",
    "print(f\"Generating with LoRA-trained model: '{test_prompt}'\")\n",
    "lora_image = m.generate(\n",
    "    test_prompt,\n",
    "    num_inference_steps=4,\n",
    "    guidance_scale=0.0,\n",
    ")\n",
    "lora_image = lora_image[0] if isinstance(lora_image, list) else lora_image\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(base_image)\n",
    "axes[0].set_title(\"Base Model (No LoRA)\", fontsize=12)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(lora_image)\n",
    "axes[1].set_title(\"With Trained LoRA\", fontsize=12)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Prompt: '{test_prompt}'\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Comparison saved to comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "- ✅ Loaded a dataset from HuggingFace\n",
    "- ✅ Trained a LoRA in just 5 lines of code\n",
    "- ✅ Generated images with your custom LoRA\n",
    "\n",
    "### Try These Next:\n",
    "\n",
    "1. **Use your own images**: Replace the dataset with your own image folder\n",
    "2. **Train longer**: Increase `steps` to 500-1000 for better results\n",
    "3. **Try different models**: Use `stabilityai/stable-diffusion-xl-base-1.0` or `black-forest-labs/FLUX.1-dev`\n",
    "4. **Adjust LoRA parameters**: Experiment with `rank` and `alpha`\n",
    "5. **Serve with API**: Use `hypergen serve` to deploy your model\n",
    "\n",
    "### Resources:\n",
    "- [HyperGen Documentation](https://github.com/ntegrals/hypergen)\n",
    "- [Examples Directory](../examples/)\n",
    "- [HuggingFace Diffusers](https://huggingface.co/docs/diffusers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
